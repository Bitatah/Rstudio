\documentclass[
  man,
  floatsintext,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[bidi=default]{babel}
\babelprovide[main,import]{american}


% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}

\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\usepackage{subcaption}


\renewcommand\thesubfigure{\Alph{subfigure}}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}



\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}





\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{Handling Duplicates in Data}


\shorttitle{Duplicate Data}


\usepackage{etoolbox}






\author{Bita Taheri (Matriculation:400889819)}



\affiliation{
{Hochschule Fresenius - University of Applied Science}}




\leftheader{(Matriculation:400889819)}



\abstract{Duplicate data entries are a common issue in data analysis and
can significantly impact the accuracy and reliability of analytical
results. This report discusses the nature of duplicates, their different
types, and the challenges they pose, such as skewed statistics, biased
models, and inefficient resource usage. It highlights best practices for
identifying and handling duplicates using both base R and the tidyverse
(especially the dplyr package). Practical examples using the built-in
iris dataset demonstrate how to detect, inspect, and remove duplicate
rows. By applying tools like duplicated(), unique(), and distinct(),
analysts can ensure clean, trustworthy data that leads to valid
insights. The report emphasizes that handling duplicates is a critical
step in the data cleaning process, essential for high-quality and
reproducible analysis. }

\keywords{Data Cleaning, Duplicate Records, Data
Quality, dplyr, Tidyverse, Data Preprocessing, Data Analysis, Data
Integrity, nBase R, Data Visualization}

\authornote{ 
\par{ }
\par{   The authors have no conflicts of interest to disclose.    }
\par{Correspondence concerning this article should be addressed to Bita
Taheri
(Matriculation:400889819), Email: \href{mailto:taheri.bita@stud.hs-fresenius.de}{taheri.bita@stud.hs-fresenius.de}}
}

\makeatletter
\let\endoldlt\endlongtable
\def\endlongtable{
\hline
\endoldlt
}
\makeatother

\urlstyle{same}



\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

%% Disable longtable counter
%% https://tex.stackexchange.com/a/248395/211326

\usepackage{etoolbox}

\makeatletter
\patchcmd{\LT@caption}
  {\bgroup}
  {\bgroup\global\LTpatch@captiontrue}
  {}{}
\patchcmd{\longtable}
  {\par}
  {\par\global\LTpatch@captionfalse}
  {}{}
\apptocmd{\endlongtable}
  {\ifLTpatch@caption\else\addtocounter{table}{-1}\fi}
  {}{}
\newif\ifLTpatch@caption
\makeatother

\begin{document}

\maketitle

\hypertarget{toc}{}
\tableofcontents
\newpage
\section[Introduction]{Handling Duplicates in Data}

\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\setlength\LTleft{0pt}


\section{introduction}\label{introduction}

In data analysis, duplicate entries refer to records that appear more
than once in a dataset. These can arise unintentionally due to data
entry errors, merging datasets, or system glitches, and if left
unchecked they can distort analytical results . Duplicates give undue
weight to certain observations, potentially skewing statistics and
leading to misleading conclusions. . For example, duplicate customer
records might inflate sales totals or duplicate experimental
measurements could bias averages. Beyond statistical distortion,
duplicates also waste storage space and computational resources, and can
complicate data management. Therefore, identifying and handling
duplicates is a critical step in data cleaning to ensure the integrity
of any analysis.

This report explores what duplicates are and why they are problematic,
the different types of duplicates, the challenges they pose in analysis,
and demonstrates how to detect and resolve duplicates in R. We will also
walk through a real-world example in RStudio, showing step-by-step how
to identify and address duplicates, and conclude with best practices.
The guidance and code examples are aimed at readers with basic R
knowledge and make use of authoritative resources such as R for Data
Science and official R documentation for reference.

\textbf{Types of Duplicates in Data}

Not all duplicates are the same -- it's important to distinguish their
types to handle them appropriately. Common categories include:

Not all duplicates are the same, and understanding their types is key to
handling them correctly. The main categories include:

\begin{itemize}
\item
  Exact Duplicates: Rows that are completely identical across all
  columns. These usually result from data entry errors or merging
  datasets. They are typically unintentional and should be removed.
\item
  Partial (Near) Duplicates: Records that share key fields (like ID or
  name) but differ slightly in other values, such as formatting or
  timestamps. These are harder to detect and may require custom rules or
  fuzzy matching.
\item
  Intentional vs.~Unintentional Duplicates: Some duplicates are valid,
  like repeated measurements in longitudinal studies or sales logs.
  These should not be removed but analyzed properly (e.g.~aggregated or
  paired).
\end{itemize}

In contrast, unintentional duplicates---such as repeated entries due to
copy-paste errors---should usually be eliminated.

Even intentional duplicates can cause problems if not handled carefully.
Always evaluate duplicates in context to decide whether to keep,
combine, or drop them.

Why Duplicates Are Problematic: Analytical Challenges

Duplicate records in a dataset can seriously affect the quality and
accuracy of data analysis. Key problems include:

\begin{itemize}
\item
  Skewed Statistics: Duplicates inflate metrics like totals, means, and
  standard deviations, leading to inaccurate results.
\item
  Misleading Visuals: Charts and graphs may appear distorted due to
  repeated values, making the data look skewed or clustered when it's
  not.
\item
  Model Bias: In predictive modeling, duplicates can cause overfitting
  by giving too much weight to certain patterns, which reduces model
  reliability.
\item
  False Significance: Duplicates can exaggerate correlations and affect
  hypothesis testing by violating the assumption of independent
  observations.
\item
  Wasted Resources: Extra data increases storage needs and slows down
  processing, especially in large datasets.
\item
  Data Quality Issues: Unexpected duplicates often signal deeper
  problems like flawed data entry or merging errors.
\end{itemize}

In short, duplicates must be identified and carefully handled to ensure
valid, efficient, and trustworthy analysis.

\section{Detecting and Handling Duplicates in
R}\label{detecting-and-handling-duplicates-in-r}

R provides robust tools for identifying and removing duplicates, both in
base R and in the tidyverse collection of packages. This section details
how to use these tools with code examples.

We will cover base R functions like duplicated(), unique(), and
anyDuplicated(), as well as tidyverse approaches with dplyr (especially
the distinct()function).

Conceptual illustration of identifying and removing duplicate rows in a
dataset (blue rows indicate duplicates). In R, base functions like
duplicated()/unique() and the dplyr function distinct() are commonly
used to address duplicates.

\textbf{Base R Techniques for Duplicates}

Base R has built-in functions to detect duplicates and extract unique
values. The primary function is duplicated(), which returns a logical
vector indicating which elements or rows are duplicates of a previous
occurrencerdrr.io. Each element of the vector is TRUE if that
row/element has appeared before and is thus a duplicate, and FALSE if it
is the first occurrence or unique. A related base function, unique(),
returns the unique elements or rows of a vector/data frame, effectively
removing duplicates. Let's start with a simple example on a vector to
illustrate duplicated():

\section{A numeric vector with some
repeats}\label{a-numeric-vector-with-some-repeats}

x \textless- c(1, 1, 4, 5, 4, 6)

\section{Identify which elements are
duplicates}\label{identify-which-elements-are-duplicates}

duplicated(x) \#\textgreater{} {[}1{]} FALSE TRUE FALSE FALSE TRUE FALSE

Here, the output shows TRUE for positions 2 and 5, meaning x{[}2{]} and
x{[}5{]} are duplicates of earlier values in the vector (indeed,
x{[}2{]} = 1 is a repeat of x{[}1{]} = 1, and x{[}5{]} = 4 repeats
x{[}3{]} = 4). The first occurrence of a value is not marked as a
duplicate -- by definition, duplicated() only flags the subsequent
occurrences. We can use this result to extract or remove duplicates: \#
Extract the duplicate values themselves x{[}duplicated(x){]}
\#\textgreater{} {[}1{]} 1 4

\section{Extract the unique values (remove
duplicates)}\label{extract-the-unique-values-remove-duplicates}

x{[}!duplicated(x){]} \#\textgreater{} {[}1{]} 1 4 5 6

In the above, x{[}duplicated(x){]} returns the values that had appeared
before (1 and 4 in this case). Conversely, prefixing with ! (logical
NOT) gives the values that are not duplicates, effectively the unique
set \{1, 4, 5, 6\}. Using !duplicated()on a data frame will filter to
unique rows. In fact, the base R unique() function achieves the same:
unique(x) would yield c(1, 4, 5, 6) in this example, and
unique(my\_data\_frame) removes duplicate rows from a data frame. For
data frames, duplicated() operates row-wise. For example, if we have a
data frame df, duplicated(df) will return a logical vector of length
equal to the number of rows, with TRUE for each row that is identical to
a previous row. We can use this to identify and remove duplicate rows.
Consider a data frame df and the pattern:

dup\_rows \textless- df{[}duplicated(df), {]} \# subset of all duplicate
rows (beyond first occurrences) df\_no\_dups \textless-
df{[}!duplicated(df), {]} \# dataframe with duplicates removed

The first line collects all rows that are duplicates (this will exclude
the first instance of each set of duplicates). The second line keeps
only the rows that are not flagged as duplicates, which is equivalent to
unique(df)

Partial duplicates by column: Sometimes we want to detect duplicates
based on a subset of columns (for instance, find repeated IDs regardless
of other differences). In base R, one can use the duplicated() function
on a specific column or a combination of columns. For example,
duplicated(df\$ID) would flag duplicate IDs in the data frame. We can
also use duplicated(df{[}, c(``col1'',``col2''){]}) to check for
duplicates with respect to a combination of col1 and col2. Rows where
both col1 and col2 match an earlier row will be marked. As an example,
using R's built-in iris dataset (which has columns for flower
measurements and species):

\section{Remove duplicate rows based on a single column, e.g.,
Sepal.Width}\label{remove-duplicate-rows-based-on-a-single-column-e.g.-sepal.width}

iris\_unique\_width \textless- iris{[} !duplicated(iris\$Sepal.Width),
{]}

This keeps only the first occurrence of each Sepal.Width value and drops
any later rows that repeat a Sepal.Width seen before. After this
operation, iris\_unique\_width would have one row per unique sepal width
measurement (in iris, that yields 23 rows out of 150, as there were many
repeated widths). Similarly, one could use multiple columns by
constructing a key, e.g., !duplicated(iris{[},
c(``Sepal.Length'',``Species''){]}) to remove rows where both length and
species repeat. R also provides the helper function anyDuplicated(),
which is essentially a faster check to see if any duplicates exist in an
object. It returns the index of the first duplicate found (or 0 if no
duplicates). This is useful for a quick confirmation. For instance:

anyDuplicated(iris) \#\textgreater{} {[}1{]} 143

The result ``143'' (hypothetical output) indicates that the 143rd row of
iris is a duplicate of an earlier row (meaning at least one duplicate
exists). If the result were 0, it would mean no duplicates. Using
which(duplicated(iris)) would list all indices of duplicate rows, not
just the first. In summary, base R's approach to duplicates typically
involves using duplicated() to create a logical index, then subsetting
the data. While effective, it can be a bit clunky for complex operations
(like deciding which duplicates to keep based on some criteria). That's
where the tidyverse tools often offer more convenience.

Of course! Here's a \textbf{longer and more detailed summary} of the
section on \textbf{Tidyverse Techniques (dplyr)} for handling
duplicates, without being too lengthy:

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Tidyverse Techniques (dplyr) -- Summary}

In the \textbf{tidyverse}, the \texttt{dplyr} package offers a clear and
powerful way to detect and remove duplicates using the
\texttt{distinct()} function. It works similarly to base R's
\texttt{unique()}, but is often more efficient and
user-friendly---especially when working with data frames.

🔹 How \texttt{distinct()} Works:

\begin{itemize}
\tightlist
\item
  When you run \texttt{distinct(df)}, it returns a new data frame where
  \textbf{duplicate rows are removed}, keeping \textbf{only the first
  occurrence} of each unique row.
\item
  The \textbf{original order of rows is preserved}, and only the later
  repeated ones are dropped.
\end{itemize}

🔹 Focusing on Specific Columns:

You don't have to consider the whole dataset. You can apply
\texttt{distinct()} to specific columns to check for uniqueness in part
of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{distinct}\NormalTok{(df, column1, column2, }\AttributeTok{.keep\_all =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This keeps \textbf{one row per unique combination} of the selected
  columns.
\item
  The option \texttt{.keep\_all\ =\ TRUE} tells R to keep the entire row
  (not just the selected columns).
\item
  If you leave \texttt{.keep\_all\ =\ FALSE} (the default), only the
  columns you list will be returned.
\end{itemize}

🔹 Example Using \texttt{iris} Dataset:

The iris dataset has 150 rows. Here's how you can check and remove
duplicates:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{iris\_unique }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{distinct}\NormalTok{()}
\FunctionTok{nrow}\NormalTok{(iris\_unique)}
\CommentTok{\# Output: 149}
\end{Highlighting}
\end{Shaded}

This confirms that there is \textbf{one duplicate row}, and it has been
removed. The \texttt{distinct()} function simplifies this task with just
one line of code.

🔹 Using \texttt{distinct()} on Subsets of Data:

You can find unique combinations based on just a few columns. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris\_species\_lengths }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{(Species, Petal.Length, }\AttributeTok{.keep\_all =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This returns only the \texttt{Species} and \texttt{Petal.Length} columns
with unique combinations, ignoring other variables.

🔹 Counting Duplicates:

Another useful approach is to \textbf{count how many times each row or
combination appears}, then filter only those with duplicates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This groups by all key columns and returns rows that appear more than
once, showing exactly how many times each duplicated row is repeated.

If you just want the \textbf{number} of duplicated rows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Base R}
\FunctionTok{sum}\NormalTok{(}\FunctionTok{duplicated}\NormalTok{(iris))}

\CommentTok{\# Or with dplyr}
\FunctionTok{nrow}\NormalTok{(iris) }\SpecialCharTok{{-}} \FunctionTok{nrow}\NormalTok{(}\FunctionTok{distinct}\NormalTok{(iris))}
\end{Highlighting}
\end{Shaded}

Both approaches will return \texttt{1} for the iris dataset, because it
contains one duplicate.

🔹 Other Useful Tools:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{janitor::get\_dupes()}}: A handy function from the
  \texttt{janitor} package that lists duplicate rows and how often each
  occurs.
\item
  \textbf{\texttt{data.table} package}: Offers very fast tools for
  handling duplicates in large datasets.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Summary of Benefits:

\begin{itemize}
\tightlist
\item
  \texttt{distinct()} is easy to read and integrate into data cleaning
  pipelines using \texttt{\%\textgreater{}\%}.
\item
  It works for full data frames or selected columns.
\item
  It's more efficient and consistent than some base R alternatives.
\item
  Combined with \texttt{count()}, it helps \textbf{inspect}, not just
  remove, duplicates.
\end{itemize}

Refer to these sources to get more information on data duplication.
(\citeproc{ref-datanovia2020duplicates}{Datanovia, 2020})

(\citeproc{ref-sanderson2024duplicated}{Sanderson, 2024})

(\citeproc{ref-sscc2020duplicates}{Social Science Computing Cooperative,
2020})

\section{Affidavit}\label{affidavit}

I hereby affirm that this submitted paper was authored unaided and
solely by me. Additionally, no other sources than those in the reference
list were used. Parts of this paper, including tables and figures, that
have been taken either verbatim or analogously from other works have in
each case been properly cited with regard to their origin and
authorship. This paper either in parts or in its entirety, be it in the
same or similar form, has not been submitted to any other examination
board and has not been published.

I acknowledge that the university may use plagiarism detection software
to check my thesis. I agree to cooperate with any investigation of
suspected plagiarism and to provide any additional information or
evidence requested by the university.

Checklist:

\begin{itemize}
\tightlist
\item[$\square$]
  The handout contains 3-5 pages of text.
\item[$\square$]
  The submission contains the Quarto file of the handout.
\item[$\square$]
  The submission contains the Quarto file of the presentation.
\item[$\square$]
  The submission contains the HTML file of the handout.
\item[$\square$]
  The submission contains the HTML file of the presentation.
\item[$\square$]
  The submission contains the PDF file of the handout.
\item[$\square$]
  The submission contains the PDF file of the presentation.
\item[$\square$]
  The title page of the presentation and the handout contain personal
  details (name, email, matriculation number).
\item[$\square$]
  The handout contains a abstract.
\item[$\square$]
  The presentation and the handout contain a bibliography, created using
  BibTeX with APA citation style.
\item[$\square$]
  Either the handout or the presentation contains R code that proof the
  expertise in coding.
\item[$\square$]
  The handout includes an introduction to guide the reader and a
  conclusion summarizing the work and discussing potential further
  investigations and readings, respectively.
\item[$\square$]
  All significant resources used in the report and R code development.
\item[$\square$]
  The filled out Affidavit.
\item[$\square$]
  A concise description of the successful use of Git and GitHub, as
  detailed here: \url{https://github.com/hubchev/make_a_pull_request}.
\item[$\square$]
  The link to the presentation and the handout published on GitHub.
\end{itemize}

{[}Bita Taheri,{]} {[}06/04/2025,{]} {[}Koln{]}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-datanovia2020duplicates}
Datanovia. (2020). \emph{Identify and remove duplicate data in r}.
\url{https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/}.
\url{https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/}

\bibitem[\citeproctext]{ref-sanderson2024duplicated}
Sanderson, S. (2024). \emph{How to use the duplicated function in base
r}.
\url{https://www.r-bloggers.com/2024/01/how-to-use-the-duplicated-function-in-base-r/}.
\url{https://www.r-bloggers.com/2024/01/how-to-use-the-duplicated-function-in-base-r/}

\bibitem[\citeproctext]{ref-sscc2020duplicates}
Social Science Computing Cooperative. (2020). \emph{Data wrangling
essentials: Section 4.9 - duplicate observations}.
\url{https://sscc.wisc.edu/sscc/pubs/dwr/duplicates.htm}.
\url{https://sscc.wisc.edu/sscc/pubs/dwr/duplicates.htm}

\end{CSLReferences}






\end{document}
